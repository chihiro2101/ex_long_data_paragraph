Reliability (statistics)

==Item response theory==
It was well known to classical test theorists that measurement precision is not uniform across the scale of measurement. Tests tend to distinguish better for test-takers with moderate trait levels and worse among high- and low-scoring test-takers. Item response theory extends the concept of reliability from a single index to a function called the information function. The IRT information function is the inverse of the conditional observed score standard error at any given test score.
==Estimation==
The goal of estimating reliability is to determine how much of the variability in test scores is due to errors in measurement and how much is due to variability in true scores.
Four practical strategies have been developed that provide workable methods of estimating test reliability.<ref name =David />
1. Test-retest reliability method: directly assesses the degree to which test scores are consistent from one test administration to the next.
It involves:
* Administering a test to a group of individuals
* Re-administering the same test to the same group at some later time
* Correlating the first set of scores with the second
The correlation between scores on the first test and the scores on the retest is used to estimate the reliability of the test using the Pearson product-moment correlation coefficient: see also item-total correlation.
2. Parallel-forms method:
The key to this method is the development of alternate test forms that are equivalent in terms of content, response processes and statistical characteristics. For example, alternate forms exist for several tests of general intelligence, and these tests are generally seen equivalent.<ref name =David />
With the parallel test model it is possible to develop two forms of a test that are equivalent in the sense that a person's true score on form A would be identical to their true score on form B. If both forms of the test were administered to a number of people, differences between scores on form A and form B may be due to errors in measurement only.<ref name =David />
It involves:
* Administering one form of the test to a group of individuals
* At some later time, administering an alternate form of the same test to the same group of people
* Correlating scores on form A with scores on form B
The correlation between scores on the two alternate forms is used to estimate the reliability of the test.
This method provides a partial solution to many of the problems inherent in the test-retest reliability method. For example, since the two forms of the test are different, carryover effect is less of a problem. Reactivity effects are also partially controlled; although taking the first test may change responses to the second test. However, it is reasonable to assume that the effect will not be as strong with alternate forms of the test as with two administrations of the same test.<ref name =David />
However, this technique has its disadvantages:
* It may be very difficult to create several alternate forms of a test
* It may also be difficult if not impossible to guarantee that two alternate forms of a test are parallel measures
3. Split-half method:
This method treats the two halves of a measure as alternate forms. It provides a simple solution to the problem that the parallel-forms method faces: the difficulty in developing alternate forms.<ref name =David />
It involves:
* Administering a test to a group of individuals
* Splitting the test in half
* Correlating scores on one half of the test with scores on the other half of the test
The correlation between these two split halves is used in estimating the reliability of the test. This halves reliability estimate is then stepped up to the full test length using the Spearman–Brown prediction formula.
There are several ways of splitting a test to estimate reliability. For example, a 40-item vocabulary test could be split into two subtests, the first one made up of items 1 through 20 and the second made up of items 21 through 40. However, the responses from the first half may be systematically different from responses in the second half due to an increase in item difficulty and fatigue.<ref name =David />
In splitting a test, the two halves would need to be as similar as possible, both in terms of their content and in terms of the probable state of the respondent. The simplest method is to adopt an odd-even split, in which the odd-numbered items form one half of the test and the even-numbered items form the other. This arrangement guarantees that each half will contain an equal number of items from the beginning, middle, and end of the original test.<ref name =David />
4. Internal consistency: assesses the consistency of results across items within a test. The most common internal consistency measure is Cronbach's alpha, which is usually interpreted as the mean of all possible split-half coefficients.<ref name="Cortina">Cortina, J.M., (1993). What Is Coefficient Alpha? An Examination of Theory and Applications. Journal of Applied Psychology, 78(1), 98–104.</ref> Cronbach's alpha is a generalization of an earlier form of estimating internal consistency, Kuder–Richardson Formula 20.<ref name="Cortina" /> Although the most commonly used, there are some misconceptions regarding Cronbach's alpha.<ref>Ritter, N. (2010). Understanding a widely misunderstood statistic: Cronbach's alpha. Paper presented at Southwestern Educational Research Association (SERA) Conference 2010, New Orleans, LA (ED526237).</ref><ref></ref>
These measures of reliability differ in their sensitivity to different sources of error and so need not be equal. Also, reliability is a property of the scores of a measure rather than the measure itself and are thus said to be sample dependent. Reliability estimates from one sample might differ from those of a second sample (beyond what might be expected due to sampling variations) if the second sample is drawn from a different population because the true variability is different in this second population. (This is true of measures of all types—yardsticks might measure houses well yet have poor reliability when used to measure the lengths of insects.)
Reliability may be improved by clarity of expression (for written assessments), lengthening the measure,<ref name="Cortina" /> and other informal means. However, formal psychometric analysis, called item analysis, is considered the most effective way to increase reliability. This analysis consists of computation of item difficulties and item discrimination indices, the latter index involving computation of correlations between the items and sum of the item scores of the entire test. If items that are too difficult, too easy, and/or have near-zero or negative discrimination are replaced with better items, the reliability of the measure will increase.
* <math>R(t) = 1 - F(t).</math>
* <math>R(t) = \exp(-\lambda t).</math> (where <math>\lambda</math> is the failure rate)
==See also==
* Coefficient of variation
* Congeneric reliability
* Consistency (statistics)
* Homogeneity (statistics)
* Test-retest reliability
* Internal consistency
* Levels of measurement
* Accuracy and precision
* Reliability disambiguation page
* Reliability theory
* Reliability engineering
* Reproducibility
* Validity (statistics)
==References==
==External links==
* urltoken Internal and external reliability and validity explained.
* urltoken Uncertainty models, uncertainty quantification, and uncertainty processing in engineering
* urltoken The relationships between correlational and internal consistency concepts of test reliability
* urltoken The problem of negative reliabilities
pl:Rzetelność (metodologia nauki)#Rzetelność w psychometrii

</math>
Unfortunately, there is no way to directly observe or calculate the true score, so a variety of methods are used to estimate the reliability of a test.
Some examples of the methods to estimate reliability include test-retest reliability, internal consistency reliability, and parallel-test reliability. Each method comes at the problem of figuring out the source of error in the test somewhat differently.