Primality test

== Simple methods ==
The simplest primality test is trial division: given an input number, n,
check whether it is evenly divisible by any prime number between 2 and (i.e. that the division leaves no remainder). If so, then n is composite. Otherwise, it is prime.<ref name="Riesel2-3">Riesel (1994) pp.2-3</ref>
For example, consider the number 100, which is evenly divisible by these numbers:
:2, 4, 5, 10, 20, 25, 50
Note that the largest factor, 50, is half of 100. This holds true for all n: all divisors are less than or equal to n/2.
Actually, when we test all possible divisors up to n/2, we will discover some factors twice. To observe this, rewrite the list of divisors as a list of products, each equal to 100:
:2 × 50,&nbsp;&nbsp; 4 × 25,&nbsp;&nbsp; 5 × 20,&nbsp;&nbsp; 10 × 10,&nbsp;&nbsp; 20 × 5,&nbsp;&nbsp; 25 × 4,&nbsp;&nbsp; 50 × 2
Notice that products past 10 x 10 merely repeat numbers which appeared in earlier products. For example, 5 x 20 and 20 x 5 consist of the same numbers. This holds true for all n: all unique divisors of n are numbers less than or equal to , so we need not search past that.<ref name="Riesel2-3"/> (In this example, = = 10.)
All even numbers greater than 2 can also be eliminated since, if an even number can divide n, so can 2.
Let's use trial division to test the primality of 17. We need only test for divisors up to , i.e. integers less than or equal to <math>\scriptstyle \sqrt</math>.
=== Other tests ===
Leonard Adleman and Ming-Deh Huang presented an errorless (but expected polynomial-time) variant of the elliptic curve primality test. Unlike the other probabilistic tests, this algorithm produces a primality certificate, and thus can be used to prove that a number is prime.<ref name=AH92></ref> The algorithm is prohibitively slow in practice.
If quantum computers were available, primality could be tested Big O notation than by using classical computers. A combination of Shor's algorithm, an integer factorization method, with the Pocklington primality test could solve the problem in <math>O((\log n)^3 (\log\log n)^2 \log\log\log n)</math>.<ref></ref>
== Fast deterministic tests ==
Near the beginning of the 20th century, it was shown that a corollary of Fermat's little theorem could be used to test for primality.<ref></ref> This resulted in the Pocklington primality test.<ref></ref> However, as this test requires a partial factorization of n&nbsp;−&nbsp;1 the running time was still quite slow in the worst case. The first deterministic primality test significantly faster than the naive methods was the cyclotomy test; its runtime can be proven to be O((log&nbsp;n)<sup>c&nbsp;log&nbsp;log&nbsp;log&nbsp;n</sup>), where n is the number to test for primality and c is a constant independent of n. Many further improvements were made, but none could be proven to have polynomial running time. (Note that running time is measured in terms of the size of the input, which in this case is ~ log&nbsp;n, that being the number of bits needed to represent the number n.) The elliptic curve primality test can be proven to run in O((log&nbsp;n)<sup>6</sup>), if some conjectures on analytic number theory are true. Similarly, under the generalized Riemann hypothesis, the deterministic Miller's test, which forms the basis of the probabilistic Miller–Rabin test, can be proved to run in Õ((log&nbsp;n)<sup>4</sup>).<ref></ref> In practice, this algorithm is slower than the other two for sizes of numbers that can be dealt with at all. Because the implementation of these two methods is rather difficult and creates a risk of programming errors, slower but simpler tests are often preferred.
In 2002, the first provably unconditional deterministic polynomial time test for primality was invented by Manindra Agrawal, Neeraj Kayal, and Nitin Saxena. The AKS primality test runs in Õ((log&nbsp;n)<sup>12</sup>) (improved to Õ((log&nbsp;n)<sup>7.5</sup>)<ref name=":0"></ref> in the published revision of their paper), which can be further reduced to Õ((log&nbsp;n)<sup>6</sup>) if the Sophie Germain prime is true.<ref name="AKS"></ref> Subsequently, Lenstra and Pomerance presented a version of the test which runs in time Õ((log&nbsp;n)<sup>6</sup>) unconditionally.<ref></ref>
Agrawal, Kayal and Saxena suggest a variant of their algorithm which would run in Õ((log&nbsp;n)<sup>3</sup>) if Agrawal's conjecture is true; however, a heuristic argument by Hendrik Lenstra and Carl Pomerance suggests that it is probably false.<ref name=":0" /> A modified version of the Agrawal's conjecture, the Agrawal–Popovych conjecture,<ref></ref> may still be true.
== Complexity ==
In computational complexity theory, the formal language corresponding to the prime numbers is denoted as PRIMES. It is easy to show that PRIMES is in Co-NP: its complement COMPOSITES is in NP because one can decide compositeness by nondeterministically guessing a factor.
In 1975, Vaughan Pratt showed that there existed a certificate for primality that was checkable in polynomial time, and thus that PRIMES was in NP, and therefore in NP&nbsp;∩&nbsp;coNP. See primality certificate for details.
The subsequent discovery of the Solovay–Strassen and Miller–Rabin algorithms put PRIMES in coRP. In 1992, the Adleman–Huang algorithm<ref name=AH92/> reduced the complexity to ZPP = RP&nbsp;∩&nbsp;coRP, which superseded Pratt's result.
The Adleman–Pomerance–Rumely primality test from 1983 put PRIMES in QP (quasi-polynomial time), which is not known to be comparable with the classes mentioned above.
Because of its tractability in practice, polynomial-time algorithms assuming the Riemann hypothesis, and other similar evidence, it was long suspected but not proven that primality could be solved in polynomial time. The existence of the AKS primality test finally settled this long-standing question and placed PRIMES in P. However, PRIMES is not known to be P-complete, and it is not known whether it lies in classes lying inside P such as NC or L. It is known that PRIMES is not in AC0.<ref>E. Allender, M. Saks, and I.E. Shparlinski, A lower bound for primality, J. Comp. Syst. Sci. 62 (2001), pp. 356–366.</ref>
== Number-theoretic methods ==
Certain number-theoretic methods exist for testing whether a number is prime, such as the Lucas test and Proth's test. These tests typically require factorization of n&nbsp;+&nbsp;1, n − 1, or a similar quantity, which means that they are not useful for general-purpose primality testing, but they are often quite powerful when the tested number n is known to have a special form.
The Lucas test relies on the fact that the multiplicative order of a number a modulo n is n − 1 for a prime n when a is a primitive root modulo n. If we can show a is primitive for n, we can show n is prime.

A primality test is an algorithm for determining whether an input number is prime. Among other fields of mathematics, it is used for cryptography. Unlike integer factorization, primality tests do not generally give prime factors, only stating whether the input number is prime or not. Factorization is thought to be a computationally difficult problem, whereas primality testing is comparatively easy (its running time is polynomial in the size of the input). Some primality tests prove that a number is prime, while others like Miller–Rabin prove that a number is composite. Therefore, the latter might more accurately be called compositeness tests instead of primality tests.